{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMj/fkM7P22eIVZ4LhJH6aB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshyadav1508/GGUF_Quantization_of_LLM/blob/main/gguf_quant_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LkyRM1DJu67"
      },
      "outputs": [],
      "source": [
        "# Install llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"Qwen/Qwen1.5-1.8B\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]"
      ],
      "metadata": {
        "id": "I2ZqHWcCKf43"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"our MODEL_NAME is : {MODEL_NAME}\")\n",
        "print(f\"our MODEL_ID is : {MODEL_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FWx4OasK-wm",
        "outputId": "d16d0ee9-dda6-4ecf-936b-97761f467db0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our MODEL_NAME is : Qwen1.5-1.8B\n",
            "our MODEL_ID is : Qwen/Qwen1.5-1.8B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ7DzseuLLd2",
        "outputId": "d465b38f-3eda-4e86-b7e1-b520eb4e9593"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Qwen1.5-1.8B'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 70 (delta 31), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (70/70), 3.61 MiB | 4.99 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "print(f\"our fp16 model will be : {fp16}\\n\")\n",
        "\n",
        "!python llama.cpp/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce-vUn97LSTZ",
        "outputId": "dc8fc839-557e-4c7b-87ac-494a12bb4671"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our fp16 model will be : Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin\n",
            "\n",
            "Loading model: Qwen1.5-1.8B\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "gguf: Adding 151387 merge(s).\n",
            "gguf: Setting special token type eos to 151643\n",
            "gguf: Setting special token type pad to 151643\n",
            "gguf: Setting special token type bos to 151643\n",
            "gguf: Setting chat_template to {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Exporting model to 'Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin'\n",
            "gguf: loading model part 'model.safetensors'\n",
            "output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "token_embd.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "output_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "Model successfully exported to 'Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m968VuVjL2Zr",
        "outputId": "9babd361-dc18-4a48-c62b-5f34431a79de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 2442 (d84c4850)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin' to 'Qwen1.5-1.8B/qwen1.5-1.8b.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-1.8B\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 5504\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type  f16:  170 tensors\n",
            "llama_model_quantize_internal: meta size = 5945120 bytes\n",
            "[   1/ 291]                        output.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   2/ 291]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q4_K .. size =   593.50 MiB ->   166.92 MiB\n",
            "[   3/ 291]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 291]                blk.0.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[   5/ 291]                blk.0.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[   6/ 291]                  blk.0.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[   7/ 291]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 291]                    blk.0.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   9/ 291]                  blk.0.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  10/ 291]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  11/ 291]                    blk.0.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  12/ 291]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  13/ 291]                    blk.0.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 291]                  blk.0.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  15/ 291]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  16/ 291]                blk.1.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  17/ 291]                blk.1.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  18/ 291]                  blk.1.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  19/ 291]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 291]                    blk.1.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  21/ 291]                  blk.1.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  22/ 291]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  23/ 291]                    blk.1.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 291]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  25/ 291]                    blk.1.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 291]                  blk.1.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 291]               blk.10.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  29/ 291]               blk.10.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  30/ 291]                 blk.10.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  31/ 291]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 291]                   blk.10.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  33/ 291]                 blk.10.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  34/ 291]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]                   blk.10.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  36/ 291]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  37/ 291]                   blk.10.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 291]                 blk.10.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  39/ 291]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 291]               blk.11.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  41/ 291]               blk.11.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  42/ 291]                 blk.11.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  43/ 291]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 291]                   blk.11.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  45/ 291]                 blk.11.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  46/ 291]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  47/ 291]                   blk.11.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  48/ 291]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  49/ 291]                   blk.11.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 291]                 blk.11.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  51/ 291]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  52/ 291]               blk.12.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  53/ 291]               blk.12.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  54/ 291]                 blk.12.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  55/ 291]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 291]                   blk.12.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 291]                 blk.12.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  58/ 291]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  59/ 291]                   blk.12.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  60/ 291]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  61/ 291]                   blk.12.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 291]                 blk.12.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  63/ 291]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  64/ 291]               blk.13.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  65/ 291]               blk.13.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  66/ 291]                 blk.13.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  67/ 291]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 291]                   blk.13.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  69/ 291]                 blk.13.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  70/ 291]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]                   blk.13.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 291]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  73/ 291]                   blk.13.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 291]                 blk.13.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  75/ 291]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 291]               blk.14.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  77/ 291]               blk.14.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  78/ 291]                 blk.14.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  79/ 291]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 291]                   blk.14.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  81/ 291]                 blk.14.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  82/ 291]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  83/ 291]                   blk.14.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  84/ 291]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  85/ 291]                   blk.14.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 291]                 blk.14.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  87/ 291]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  88/ 291]               blk.15.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  89/ 291]               blk.15.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  90/ 291]                 blk.15.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[  91/ 291]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 291]                   blk.15.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  93/ 291]                 blk.15.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  94/ 291]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]                   blk.15.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  96/ 291]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  97/ 291]                   blk.15.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 291]                 blk.15.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 100/ 291]               blk.16.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 101/ 291]               blk.16.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 102/ 291]                 blk.16.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 103/ 291]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 291]                   blk.16.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 291]                 blk.16.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 291]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 291]                   blk.16.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 108/ 291]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 109/ 291]                   blk.16.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 291]                 blk.16.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 111/ 291]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 291]               blk.17.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 113/ 291]               blk.17.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 114/ 291]                 blk.17.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 115/ 291]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 291]                   blk.17.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 117/ 291]                 blk.17.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 118/ 291]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 119/ 291]                   blk.17.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 120/ 291]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 121/ 291]                   blk.17.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 291]                 blk.17.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 123/ 291]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 124/ 291]               blk.18.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 125/ 291]               blk.18.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 126/ 291]                 blk.18.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 127/ 291]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 291]                   blk.18.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 129/ 291]                 blk.18.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 130/ 291]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]                   blk.18.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 132/ 291]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 133/ 291]                   blk.18.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 291]                 blk.18.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 136/ 291]               blk.19.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 137/ 291]               blk.19.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 138/ 291]                 blk.19.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 139/ 291]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 291]                   blk.19.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 141/ 291]                 blk.19.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 142/ 291]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 143/ 291]                   blk.19.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 144/ 291]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 145/ 291]                   blk.19.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 291]                 blk.19.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 147/ 291]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 291]                blk.2.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 149/ 291]                blk.2.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 150/ 291]                  blk.2.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 151/ 291]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 291]                    blk.2.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 153/ 291]                  blk.2.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 154/ 291]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 155/ 291]                    blk.2.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 156/ 291]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 157/ 291]                    blk.2.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 158/ 291]                  blk.2.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 159/ 291]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 291]               blk.20.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 161/ 291]               blk.20.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 162/ 291]                 blk.20.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 163/ 291]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 291]                   blk.20.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 165/ 291]                 blk.20.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 166/ 291]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 167/ 291]                   blk.20.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 168/ 291]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 169/ 291]                   blk.20.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 291]                 blk.20.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 171/ 291]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 172/ 291]               blk.21.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 173/ 291]               blk.21.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 174/ 291]                 blk.21.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 175/ 291]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 176/ 291]                   blk.21.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 177/ 291]                 blk.21.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 178/ 291]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 179/ 291]                   blk.21.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 180/ 291]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 181/ 291]                   blk.21.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 291]                 blk.21.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 183/ 291]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 291]               blk.22.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 185/ 291]               blk.22.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 186/ 291]                 blk.22.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 187/ 291]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 291]                   blk.22.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 189/ 291]                 blk.22.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 190/ 291]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 191/ 291]                   blk.22.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 192/ 291]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 193/ 291]                   blk.22.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 194/ 291]                 blk.22.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 195/ 291]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 196/ 291]               blk.23.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 197/ 291]               blk.23.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 198/ 291]                 blk.23.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 199/ 291]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 291]                   blk.23.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 201/ 291]                 blk.23.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 202/ 291]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]                   blk.23.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 204/ 291]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 205/ 291]                   blk.23.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 206/ 291]                 blk.23.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 208/ 291]                blk.3.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 209/ 291]                blk.3.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 210/ 291]                  blk.3.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 211/ 291]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 212/ 291]                    blk.3.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 213/ 291]                  blk.3.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 214/ 291]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 215/ 291]                    blk.3.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 216/ 291]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 217/ 291]                    blk.3.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 218/ 291]                  blk.3.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 219/ 291]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 220/ 291]                blk.4.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 221/ 291]                blk.4.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 222/ 291]                  blk.4.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 223/ 291]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 224/ 291]                    blk.4.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 225/ 291]                  blk.4.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 226/ 291]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 227/ 291]                    blk.4.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 228/ 291]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 229/ 291]                    blk.4.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 230/ 291]                  blk.4.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 231/ 291]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 232/ 291]                blk.5.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 233/ 291]                blk.5.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 234/ 291]                  blk.5.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 235/ 291]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 236/ 291]                    blk.5.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 237/ 291]                  blk.5.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 238/ 291]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]                    blk.5.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 240/ 291]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 241/ 291]                    blk.5.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 242/ 291]                  blk.5.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 244/ 291]                blk.6.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 245/ 291]                blk.6.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 246/ 291]                  blk.6.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 247/ 291]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 291]                    blk.6.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 249/ 291]                  blk.6.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 250/ 291]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 251/ 291]                    blk.6.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 252/ 291]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 253/ 291]                    blk.6.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 254/ 291]                  blk.6.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 255/ 291]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 256/ 291]                blk.7.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 257/ 291]                blk.7.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 258/ 291]                  blk.7.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 259/ 291]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 260/ 291]                    blk.7.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 261/ 291]                  blk.7.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 262/ 291]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 263/ 291]                    blk.7.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 264/ 291]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 265/ 291]                    blk.7.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 291]                  blk.7.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 267/ 291]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 268/ 291]                blk.8.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 269/ 291]                blk.8.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 270/ 291]                  blk.8.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 271/ 291]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 272/ 291]                    blk.8.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 273/ 291]                  blk.8.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 274/ 291]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 275/ 291]                    blk.8.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 276/ 291]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 277/ 291]                    blk.8.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 278/ 291]                  blk.8.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 280/ 291]                blk.9.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 281/ 291]                blk.9.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 282/ 291]                  blk.9.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q4_K .. size =    21.50 MiB ->     6.05 MiB\n",
            "[ 283/ 291]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 284/ 291]                    blk.9.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 285/ 291]                  blk.9.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 286/ 291]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 287/ 291]                    blk.9.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 288/ 291]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 289/ 291]                    blk.9.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 290/ 291]                  blk.9.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 291/ 291]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  3503.95 MB\n",
            "llama_model_quantize_internal: quant size  =  1155.67 MB\n",
            "llama_model_quantize_internal: WARNING: 24 of 170 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 176503.63 ms\n",
            "main:    total time = 176503.63 ms\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 2442 (d84c4850)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin' to 'Qwen1.5-1.8B/qwen1.5-1.8b.Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from Qwen1.5-1.8B/qwen1.5-1.8b.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-1.8B\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 5504\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type  f16:  170 tensors\n",
            "llama_model_quantize_internal: meta size = 5945120 bytes\n",
            "[   1/ 291]                        output.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   2/ 291]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q5_K .. size =   593.50 MiB ->   204.02 MiB\n",
            "[   3/ 291]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 291]                blk.0.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[   5/ 291]                blk.0.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[   6/ 291]                  blk.0.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[   7/ 291]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 291]                    blk.0.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   9/ 291]                  blk.0.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  10/ 291]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  11/ 291]                    blk.0.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  12/ 291]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  13/ 291]                    blk.0.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 291]                  blk.0.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  15/ 291]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  16/ 291]                blk.1.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  17/ 291]                blk.1.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  18/ 291]                  blk.1.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  19/ 291]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 291]                    blk.1.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  21/ 291]                  blk.1.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  22/ 291]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  23/ 291]                    blk.1.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 291]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  25/ 291]                    blk.1.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 291]                  blk.1.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 291]               blk.10.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  29/ 291]               blk.10.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  30/ 291]                 blk.10.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  31/ 291]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 291]                   blk.10.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  33/ 291]                 blk.10.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  34/ 291]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  35/ 291]                   blk.10.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  36/ 291]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  37/ 291]                   blk.10.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 291]                 blk.10.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  39/ 291]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 291]               blk.11.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[  41/ 291]               blk.11.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  42/ 291]                 blk.11.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  43/ 291]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 291]                   blk.11.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  45/ 291]                 blk.11.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  46/ 291]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  47/ 291]                   blk.11.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  48/ 291]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  49/ 291]                   blk.11.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 291]                 blk.11.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  51/ 291]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  52/ 291]               blk.12.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[  53/ 291]               blk.12.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  54/ 291]                 blk.12.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  55/ 291]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 291]                   blk.12.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 291]                 blk.12.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  58/ 291]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  59/ 291]                   blk.12.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  60/ 291]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  61/ 291]                   blk.12.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 291]                 blk.12.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  63/ 291]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  64/ 291]               blk.13.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[  65/ 291]               blk.13.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  66/ 291]                 blk.13.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  67/ 291]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 291]                   blk.13.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  69/ 291]                 blk.13.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  70/ 291]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  71/ 291]                   blk.13.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 291]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  73/ 291]                   blk.13.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 291]                 blk.13.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  75/ 291]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 291]               blk.14.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[  77/ 291]               blk.14.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  78/ 291]                 blk.14.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  79/ 291]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 291]                   blk.14.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  81/ 291]                 blk.14.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  82/ 291]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  83/ 291]                   blk.14.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  84/ 291]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  85/ 291]                   blk.14.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 291]                 blk.14.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  87/ 291]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  88/ 291]               blk.15.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[  89/ 291]               blk.15.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  90/ 291]                 blk.15.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  91/ 291]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 291]                   blk.15.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  93/ 291]                 blk.15.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  94/ 291]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  95/ 291]                   blk.15.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  96/ 291]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  97/ 291]                   blk.15.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 291]                 blk.15.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  99/ 291]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 100/ 291]               blk.16.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 101/ 291]               blk.16.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 102/ 291]                 blk.16.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 103/ 291]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 291]                   blk.16.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 291]                 blk.16.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 106/ 291]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 107/ 291]                   blk.16.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 108/ 291]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 109/ 291]                   blk.16.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 291]                 blk.16.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 111/ 291]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 291]               blk.17.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 113/ 291]               blk.17.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 114/ 291]                 blk.17.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 115/ 291]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 291]                   blk.17.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 117/ 291]                 blk.17.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 118/ 291]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 119/ 291]                   blk.17.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 120/ 291]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 121/ 291]                   blk.17.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 291]                 blk.17.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 123/ 291]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 124/ 291]               blk.18.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 125/ 291]               blk.18.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 126/ 291]                 blk.18.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 127/ 291]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 291]                   blk.18.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 129/ 291]                 blk.18.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 130/ 291]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 131/ 291]                   blk.18.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 132/ 291]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 133/ 291]                   blk.18.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 291]                 blk.18.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 135/ 291]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 136/ 291]               blk.19.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 137/ 291]               blk.19.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 138/ 291]                 blk.19.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 139/ 291]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 291]                   blk.19.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 141/ 291]                 blk.19.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 142/ 291]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 143/ 291]                   blk.19.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 144/ 291]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 145/ 291]                   blk.19.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 291]                 blk.19.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 147/ 291]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 291]                blk.2.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 149/ 291]                blk.2.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 150/ 291]                  blk.2.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 151/ 291]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 291]                    blk.2.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 153/ 291]                  blk.2.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 154/ 291]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 155/ 291]                    blk.2.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 156/ 291]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 157/ 291]                    blk.2.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 158/ 291]                  blk.2.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 159/ 291]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 291]               blk.20.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 161/ 291]               blk.20.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 162/ 291]                 blk.20.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 163/ 291]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 291]                   blk.20.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 165/ 291]                 blk.20.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 166/ 291]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 167/ 291]                   blk.20.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 168/ 291]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 169/ 291]                   blk.20.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 291]                 blk.20.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 171/ 291]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 172/ 291]               blk.21.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 173/ 291]               blk.21.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 174/ 291]                 blk.21.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 175/ 291]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 176/ 291]                   blk.21.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 177/ 291]                 blk.21.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 178/ 291]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 179/ 291]                   blk.21.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 180/ 291]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 181/ 291]                   blk.21.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 291]                 blk.21.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 183/ 291]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 291]               blk.22.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 185/ 291]               blk.22.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 186/ 291]                 blk.22.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 187/ 291]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 291]                   blk.22.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 189/ 291]                 blk.22.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 190/ 291]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 191/ 291]                   blk.22.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 192/ 291]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 193/ 291]                   blk.22.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 194/ 291]                 blk.22.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 195/ 291]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 196/ 291]               blk.23.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 197/ 291]               blk.23.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 198/ 291]                 blk.23.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 199/ 291]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 291]                   blk.23.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 201/ 291]                 blk.23.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 202/ 291]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 203/ 291]                   blk.23.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 204/ 291]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 205/ 291]                   blk.23.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 206/ 291]                 blk.23.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 207/ 291]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 208/ 291]                blk.3.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 209/ 291]                blk.3.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 210/ 291]                  blk.3.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 211/ 291]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 212/ 291]                    blk.3.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 213/ 291]                  blk.3.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 214/ 291]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 215/ 291]                    blk.3.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 216/ 291]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 217/ 291]                    blk.3.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 218/ 291]                  blk.3.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 219/ 291]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 220/ 291]                blk.4.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 221/ 291]                blk.4.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 222/ 291]                  blk.4.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 223/ 291]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 224/ 291]                    blk.4.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 225/ 291]                  blk.4.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 226/ 291]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 227/ 291]                    blk.4.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 228/ 291]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 229/ 291]                    blk.4.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 230/ 291]                  blk.4.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 231/ 291]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 232/ 291]                blk.5.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
            "[ 233/ 291]                blk.5.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 234/ 291]                  blk.5.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 235/ 291]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 236/ 291]                    blk.5.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 237/ 291]                  blk.5.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 238/ 291]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 239/ 291]                    blk.5.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 240/ 291]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 241/ 291]                    blk.5.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 242/ 291]                  blk.5.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 243/ 291]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 244/ 291]                blk.6.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 245/ 291]                blk.6.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 246/ 291]                  blk.6.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 247/ 291]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 291]                    blk.6.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 249/ 291]                  blk.6.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 250/ 291]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 251/ 291]                    blk.6.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 252/ 291]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 253/ 291]                    blk.6.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 254/ 291]                  blk.6.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 255/ 291]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 256/ 291]                blk.7.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 257/ 291]                blk.7.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 258/ 291]                  blk.7.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 259/ 291]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 260/ 291]                    blk.7.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 261/ 291]                  blk.7.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 262/ 291]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 263/ 291]                    blk.7.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 264/ 291]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 265/ 291]                    blk.7.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 291]                  blk.7.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 267/ 291]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 268/ 291]                blk.8.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 269/ 291]                blk.8.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 270/ 291]                  blk.8.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 271/ 291]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 272/ 291]                    blk.8.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 273/ 291]                  blk.8.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 274/ 291]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 275/ 291]                    blk.8.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 276/ 291]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 277/ 291]                    blk.8.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 278/ 291]                  blk.8.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 280/ 291]                blk.9.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
            "[ 281/ 291]                blk.9.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 282/ 291]                  blk.9.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 283/ 291]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 284/ 291]                    blk.9.attn_k.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 285/ 291]                  blk.9.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 286/ 291]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 287/ 291]                    blk.9.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 288/ 291]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 289/ 291]                    blk.9.attn_v.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 290/ 291]                  blk.9.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 291/ 291]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  3503.95 MB\n",
            "llama_model_quantize_internal: quant size  =  1307.33 MB\n",
            "llama_model_quantize_internal: WARNING: 24 of 170 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 136279.37 ms\n",
            "main:    total time = 136279.37 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Qwen1.5-1.8B; du -sh qwen1*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfcQ72tAT8g7",
        "outputId": "e1b96740-7343-42f8-9ddc-a973cb637d11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5G\tqwen1.5-1.8b.fp16.bin\n",
            "1.2G\tqwen1.5-1.8b.Q4_K_M.gguf\n",
            "1.3G\tqwen1.5-1.8b.Q5_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "print(f\"gguf model which we are going to run: {model_list}\")\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    print(f\"new quatized model will be {qtype}\")\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJUwzCCbOW53",
        "outputId": "104bd19c-2abc-4038-f8da-4efc567fe487"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gguf model which we are going to run: ['qwen1.5-1.8b.Q4_K_M.gguf', 'qwen1.5-1.8b.Q5_K_M.gguf']\n",
            "Enter your prompt: funny quote\n",
            "Name of the model (options: qwen1.5-1.8b.Q4_K_M.gguf, qwen1.5-1.8b.Q5_K_M.gguf): qwen1.5-1.8b.Q4_K_M.gguf\n",
            "new quatized model will be Qwen1.5-1.8B/qwen1.5-1.8b.Q5_K_M.gguf\n",
            "Log start\n",
            "main: build = 2442 (d84c4850)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1710593447\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from Qwen1.5-1.8B/qwen1.5-1.8b.Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-1.8B\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 5504\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type q5_1:   12 tensors\n",
            "llama_model_loader: - type q8_0:   12 tensors\n",
            "llama_model_loader: - type q5_K:  133 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_layer          = 24\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5504\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 1.84 B\n",
            "llm_load_print_meta: model size       = 1.28 GiB (5.97 BPW) \n",
            "llm_load_print_meta: general.name     = Qwen1.5-1.8B\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 24 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 25/25 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   204.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1103.31 MiB\n",
            "....................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    96.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =   296.75 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   300.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.00 MiB\n",
            "llama_new_context_with_model: graph splits: 2\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33mfunny quote\u001b[0m 4th grade\n",
            "See more. . I'm not sure what you're looking for. \"I am an adult\" - A great motivational quote for children or teens, but also for adults who want to give a big 'thumbs up' and have fun with their life. Funny quotes about my school. (2) 10 of the funniest kids sayings in 2020. 53 Fun Facts About Animals That Will Make You Smile . For more information on how we protect your privacy, please read our Privacy Policy. We are here to help you get started with your website and help you grow your online\n",
            "llama_print_timings:        load time =     868.02 ms\n",
            "llama_print_timings:      sample time =     324.05 ms /   128 runs   (    2.53 ms per token,   395.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =      19.84 ms /     3 tokens (    6.61 ms per token,   151.20 tokens per second)\n",
            "llama_print_timings:        eval time =    1182.23 ms /   127 runs   (    9.31 ms per token,   107.42 tokens per second)\n",
            "llama_print_timings:       total time =    1687.74 ms /   130 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChA-wsqaQ2NW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}